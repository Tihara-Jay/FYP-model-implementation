{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCp6WXL8uAsyRLKUE9ju8+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tihara-Jay/FYP-model-implementation/blob/main/Model_Conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cy_19kYlcHu",
        "outputId": "8bb2b756-203c-43a6-b536-647bb480c449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu =  nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        self.fc2 = nn.Linear(512,1)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        x = self.fc1(pooled_output)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        logits = self.fc2(x).squeeze(-1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "h8qfyNDDlkjE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install coremltools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp563BvimimH",
        "outputId": "a5f90ee8-7d4f-4e27-b853-2277bbb34cbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting coremltools\n",
            "  Downloading coremltools-7.1-cp310-none-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from coremltools) (1.25.2)\n",
            "Requirement already satisfied: protobuf<=4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from coremltools) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from coremltools) (1.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from coremltools) (4.66.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from coremltools) (24.0)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from coremltools) (23.2.0)\n",
            "Collecting cattrs (from coremltools)\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyaml (from coremltools)\n",
            "  Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs->coremltools) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from cattrs->coremltools) (4.10.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml->coremltools) (6.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->coremltools) (1.3.0)\n",
            "Installing collected packages: pyaml, cattrs, coremltools\n",
            "Successfully installed cattrs-23.2.3 coremltools-7.1 pyaml-23.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import coremltools as ct\n",
        "import torch\n",
        "\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
        "#load the saved stAte params of the pre-trained model\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/FYP/Implementation/FYP_Implementation/Spam_image_classification.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "dummy_input_ids = torch.zeros((1, 128), dtype=torch.long)\n",
        "dummy_attention_mask = torch.zeros((1, 128), dtype=torch.long)\n",
        "traced_model = torch.jit.trace(model, (dummy_input_ids, dummy_attention_mask))\n",
        "input_shape = (1, 128)\n",
        "\n",
        "\n",
        "# Convert the model to a core ml model\n",
        "coreml_model = ct.convert(\n",
        "    traced_model,\n",
        "    source='pytorch',\n",
        "    convert_to=\"neuralnetwork\",\n",
        "    inputs=[ct.TensorType(name='input_ids', shape=input_shape),\n",
        "            ct.TensorType(name='attention_mask', shape=input_shape)],\n",
        "    outputs=['output']\n",
        ")\n",
        "\n",
        "coreml_model.save(\"/content/drive/MyDrive/FYP/Implementation/FYP_Implementation/SpamImageClassification.mlmodel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS1frTDTlpXe",
        "outputId": "dd3647f4-9990-4981-8617-0e7496bd894a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:1102: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
            "Tensor-likes are not close!\n",
            "\n",
            "Mismatched elements: 1 / 1 (100.0%)\n",
            "Greatest absolute difference: 0.16414809226989746 at index (0,) (up to 1e-05 allowed)\n",
            "Greatest relative difference: 0.15194438973092536 at index (0,) (up to 1e-05 allowed)\n",
            "  _check_trace(\n",
            "WARNING:coremltools:Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n",
            "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/635 [00:00<?, ? ops/s]WARNING:coremltools:Core ML embedding (gather) layer does not support any inputs besides the weights and indices. Those given will be ignored.\n",
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 634/635 [00:00<00:00, 2482.15 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 137.91 passes/s]\n",
            "Running MIL default pipeline: 100%|██████████| 69/69 [00:01<00:00, 65.47 passes/s]\n",
            "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 250.47 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 713/713 [00:19<00:00, 36.36 ops/s]\n"
          ]
        }
      ]
    }
  ]
}